{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this post, we will address the fundamental aspects of Torch's `Dataset`s and `DataLoader`s, considering an environment with **Data, Pipeline and Tensor Parallelism** and including functionalities to resume training after an interruption. To do this, we will introduce [`Nanoset`s](https://github.com/TJ-Solergibert/nanotron/blob/nanosets/src/nanotron/data/nanoset.py), a dataset that I have developed for training LLMs at scale for [`Nanotron`](https://github.com/TJ-Solergibert/nanotron), the 3D parallelism trainer developed by Hugging Face ðŸ¤—. \n",
    "\n",
    "# Nanosets\n",
    "Our objective for training LLMs is to build batches containing `batch_size` samples containing `sequence_length` tokens for the `input_ids` and the `labels`. To achieve this, we will present the `Nanosets`, a dataset based on [numpy memory-mapped arrays](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html), which allows to easily read bytes of data from local disk. These bytes we will read are the tokens of the documents with which we want to train our model. In order not to lengthen this article, we will start from the assumption that we already have the tokenized documents stored in a file, where each token is represented by 2 bytes.\n",
    "## Torch's Dataset\n",
    "Below we show the source code of the Nanosets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nanotron import logging\n",
    "from nanotron.data.utils import count_dataset_indexes, normalize\n",
    "from nanotron.data.nanoset import build_nanoset_index_helper\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class Nanoset(Dataset):\n",
    "    \"\"\"\n",
    "    The Nanoset dataset\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (List[str]): List of paths to tokenized datasets\n",
    "        dataset_weights (List[float]): List with the weights for weighted datasets. If None, consume all samples from all datasets without weighting. Weights are normalized in __init__\n",
    "        sequence_length (int): Sequence length of the built samples\n",
    "        train_split_num_samples (int): Number of samples the dataset needs. It's the training steps * global batch size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_paths: List[str],\n",
    "        dataset_weights: Union[List[float], None],\n",
    "        sequence_length: int,\n",
    "        train_split_num_samples: int,\n",
    "        random_seed: int = 1234,\n",
    "    ) -> None:\n",
    "\n",
    "        # Init\n",
    "        self.dataset_paths = dataset_paths\n",
    "        self.dataset_weights = dataset_weights\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_split_num_samples = train_split_num_samples\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Build Nanoset Index\n",
    "        ## To build the index we need the length of each dataset\n",
    "        self.dataset_lengths = []\n",
    "        for dataset_path in self.dataset_paths:\n",
    "            self.dataset_buffer_mmap = np.memmap(dataset_path, mode=\"r\", order=\"C\", dtype=np.uint16)\n",
    "            self.dataset_buffer = memoryview(self.dataset_buffer_mmap)\n",
    "            dataset_tokens = int(len(self.dataset_buffer))\n",
    "            number_of_samples = int(\n",
    "                dataset_tokens / sequence_length\n",
    "            )  # Discard last sample of length < sequence_length\n",
    "            self.dataset_lengths.append(number_of_samples)\n",
    "        ## Set dataset weights\n",
    "        if (\n",
    "            self.dataset_weights is None\n",
    "        ):  # Case of training with > 1 datasets without weighting them: Consume both datasets entirely on each epoch\n",
    "            self.dataset_weights = normalize(self.dataset_lengths)\n",
    "        else:\n",
    "            self.dataset_weights = normalize(dataset_weights)\n",
    "        ## Build dataset index and dataset sample index\n",
    "        self.dataset_index, self.dataset_sample_index = self.build_nanoset_index()\n",
    "\n",
    "        self.print_nanoset_info()\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns sequence_length + 1 tokens from the memmap dataset\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index into the dataset\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, numpy.ndarray]: The input ids wrapped in a dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        dataset = self.dataset_index[idx]\n",
    "        dataset_sample = self.dataset_sample_index[idx]\n",
    "\n",
    "        # Rebuild the memmap in every access to free memory\n",
    "        # https://stackoverflow.com/a/61472122\n",
    "        self.dataset_buffer_mmap = np.memmap(self.dataset_paths[dataset], mode=\"r\", order=\"C\", dtype=np.uint16)\n",
    "        self.dataset_buffer = memoryview(self.dataset_buffer_mmap)\n",
    "\n",
    "        # dtype=uint16, 2 bytes per token\n",
    "        offset = dataset_sample * self.sequence_length * 2\n",
    "        input_ids_tokens = np.frombuffer(\n",
    "            self.dataset_buffer, dtype=np.uint16, count=(self.sequence_length + 1), offset=offset\n",
    "        )\n",
    "\n",
    "        # Return tokens as np.int32 as Torch can't handle uint16\n",
    "        return {\"input_ids\": input_ids_tokens.astype(np.int32)}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: The number of samples of the Nanoset\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.dataset_index)\n",
    "\n",
    "    def build_nanoset_index(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build dataset index and dataset sample index\n",
    "        \"\"\"\n",
    "        # Compute samples per epoch and number of epochs\n",
    "        samples_per_epoch = sum(self.dataset_lengths)\n",
    "        num_epochs = int(self.train_split_num_samples / samples_per_epoch) + 1\n",
    "        # Build the dataset indexes for 1 epoch\n",
    "        dataset_index, dataset_sample_index = build_nanoset_index_helper(\n",
    "            n_samples=samples_per_epoch, weights=self.dataset_weights, dataset_sizes=self.dataset_lengths\n",
    "        )\n",
    "        # Shuffle the indexes the same way\n",
    "        numpy_random_state = np.random.RandomState(self.random_seed)\n",
    "        numpy_random_state.shuffle(dataset_index)\n",
    "        numpy_random_state = np.random.RandomState(self.random_seed)\n",
    "        numpy_random_state.shuffle(dataset_sample_index)\n",
    "        # Concatenate num_epochs the shuffled indexes\n",
    "        dataset_index = np.concatenate([dataset_index for _ in range(num_epochs)])\n",
    "        dataset_sample_index = np.concatenate([dataset_sample_index for _ in range(num_epochs)])\n",
    "        # Just keep the necessary samples\n",
    "        dataset_index = dataset_index[: self.train_split_num_samples]\n",
    "        dataset_sample_index = dataset_sample_index[: self.train_split_num_samples]\n",
    "\n",
    "        return dataset_index, dataset_sample_index\n",
    "\n",
    "    def print_nanoset_info(self):\n",
    "\n",
    "        print(f\"> Total number of samples: {len(self)}\")\n",
    "        print(f\"> Total number of tokens: {len(self) * self.sequence_length}\")\n",
    "\n",
    "        # Print samples from each dataset + weight\n",
    "        dataset_sample_count = count_dataset_indexes(self.dataset_index, len(self.dataset_paths))\n",
    "        for index, sample_count in enumerate(dataset_sample_count):\n",
    "            print(f\">   Total number of samples from the {self.dataset_paths[index].rsplit('/', 1)[-1]} dataset: {sample_count} ({round(normalize(dataset_sample_count).tolist()[index], 2)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 core aspects of a `Dataset` are:\n",
    "- **\\_\\_init\\_\\_**: This method will be responsible for creating our dataset based on the arguments we introduce. Here we build all the logic related to how we interact with the `Dataset`.\n",
    "  \n",
    "  In our case, the parameters characterizing each `Nanoset` are as follows:\n",
    "    - `dataset_paths`: One or multiple paths to the datasets containing tokenized documents.\n",
    "    - `dataset_weights`: If we train the model with >1 datasets, we can specify the proportion of samples from each one during training.\n",
    "    - `sequence_length`: Each sample of the `Nanoset` will contain `sequence_length` tokens*.\n",
    "    - `train_split_num_samples`: The minimum number of samples required to build the `Nanoset`. In case the datasets are not large enough, we will consume them repeatedly (Epochs).\n",
    "  \n",
    "  With these parameters, the first thing we will do is compute the `dataset_length` of each dataset, which will be determined by the `number_of_total_tokens / sequence_length`, discarding the last sample since its length < `sequence_length`.\n",
    "\n",
    "  Based on the `dataset_lengths`, the `dataset_weights` and the `number_of_samples_per_epoch` (defined as the `sum(dataset_lengths)`), we build the two indexes we need in order to extract samples from the `Nanoset`  ([build_nanoset_index_helper](https://github.com/TJ-Solergibert/nanotron/blob/07601a9ed89973584686639555268e1c1b52f93a/src/nanotron/data/nanoset.py#L152)):\n",
    "      \n",
    "    - `dataset_index`: Contains the index of the dataset from the list of `dataset_paths` from which to extract the sample, respecting the established dataset weight.\n",
    "        \n",
    "        ```\n",
    "        Given:\n",
    "\n",
    "        D = [d0, d1, d2, d3]        # datasets\n",
    "        DL = [8, 2, 5, 5]           # dataset lengths\n",
    "        W = [0.1, 0.5, 0.3, 0.1]    # dataset weights\n",
    "        SPE = 20                    # number of samples per epoch\n",
    "\n",
    "        Then, for example:\n",
    "\n",
    "        dataset_index = [1, 2, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 3, 1, 2, 1, 2, 1]\n",
    "        ```\n",
    "    - `dataset_sample_index`: Contains the sample index to extract from the `dataset_index[index]` dataset, always < `len(dataset)`.\n",
    "        ```\n",
    "        dataset_index =         [1, 2, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 3, 1, 2, 1, 2, 1]\n",
    "        dataset_sample_index =  [0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 3, 0, 1, 1, 4, 0, 0, 1]\n",
    "        ```\n",
    "\n",
    "      Then, we **shuffle with the same permutation both indexes** and concatenate them `number_of_epochs` times, which is defined by `train_split_num_samples` / `number_of_samples_per_epoch`.\n",
    "    \n",
    "        ```\n",
    "        Given:\n",
    "\n",
    "        N = 70                      # train split num samples\n",
    "\n",
    "        dataset_index =         [1, 2, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 3, 1, 2, 1, 2, 1]\n",
    "        dataset_sample_index =  [0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 1, 1, 3, 0, 1, 1, 4, 0, 0, 1]\n",
    "\n",
    "        Shuffle dataset_index and dataset_sample_index:\n",
    "\n",
    "        dataset_index =         [1, 1, 0, 2, 3, 1, 3, 1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 2, 2, 1]\n",
    "        dataset_sample_index =  [1, 0, 0, 4, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 3, 1, 1]\n",
    "\n",
    "        n_concatenations = (70/(20)) + 1 = 4\n",
    "        dataset_index = dataset_index concatenated 4 times\n",
    "        dataset_sample_index = dataset_sample_index concatenated 4 times\n",
    "\n",
    "        dataset_index = dataset_index[: N]\n",
    "        dataset_sample_index = dataset_sample_index[: N]\n",
    "        ```\n",
    "  \n",
    "- **\\_\\_getitem\\_\\_**: This method will be responsible of building the samples queried to the `Dataset`. Here is where we will handle all the costly operations related to each sample we want to extract, such as reading files, preprocessing or data augmentation. \n",
    "  In our case, to query the `Nanoset` for the *k-th* sample we use the `dataset_index` to retrieve the corresponding dataset from `D` and the `dataset_sample_index` to retrieve the corresponding sample from that dataset.\n",
    "  ```\n",
    "  sample = D[dataset_index[k]][dataset_sample_index[k]]\n",
    "  ```\n",
    "  As we have already mentioned, each dataset is a file where each token is represented by 2 bytes. To construct the samples, we will build a memory-mapped array from this file (`dataset_index[k]`), and from the `dataset_sample_index[k]`, we will extract `sequence_length + 1` tokens (Remember that the labels for the model training are the inputs with a 1 position shift).\n",
    "- **\\_\\_len\\_\\_**: This method will include the logic that establishes the number of samples we have in the `Dataset`. This value is essential for the `DataLoader` to know the positions it can query from the `Dataset`.\n",
    "\n",
    "The following snippet shows how to create a `Nanoset` and extract samples. It is worth to mention that we will create the **same `Nanoset`** in **each and every process** involved in the training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Total number of samples: 1000\n",
      "> Total number of tokens: 8192000\n",
      ">   Total number of samples from the yelp_review_full_input_ids.npy dataset: 1000 (1.0)\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_LENGTH = 8192\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_nanoset = Nanoset(dataset_paths=[\"datasets/yelp_review_full_input_ids.npy\"], \n",
    "                        dataset_weights=None, \n",
    "                        sequence_length=SEQUENCE_LENGTH, \n",
    "                        train_split_num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([   13,   911, 23750, ...,  2185, 34443, 16405], dtype=int32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nanoset[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch's DataLoader\n",
    "Now that we know how to build a `Dataset` and how to extract samples from it, let's move on to building the `DataLoader`. The `DataLoader` will have the help of a *collator* and a *sampler*.\n",
    "\n",
    "### The Sampler\n",
    "The *sampler* will contain the list of indices that the `DataLoader` has to extract from the `Dataset`. For this, it will create a sample index with `len(Dataset)` indices.\n",
    "\n",
    "In our case, as we will have Data Parallelism, **we must avoid consuming the same sample repeatedly during an epoch**. To do this, we will use the `DistributedSampler`, which will divide the indices of the `Dataset` by the Data Parallel size. For this, we will have to specify the Data Parallel Size (Common in all processes) and the Data Parallel Rank.\n",
    "\n",
    "> [!NOTE]\n",
    "> Due to how the training loop is designed, it expects to never exhaust all the batches that the `DataLoader` can produce. This is why, during the creation of the `Nanoset` indices, we concatenate them a sufficient number of times to never run out of samples, but before this process, we shuffle them. We shuffle the data at this moment and not in the sampler because this way, we ensure that in each epoch, we consume all the samples from each of the datasets.\n",
    "\n",
    "In the following way, we will create the `DistributedSampler` in the processes that belong to the first Data Parallel Rank with a Data Parallel Size of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "sampler = DistributedSampler(train_nanoset, num_replicas=4, rank=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be the indices that we will query from the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "sampler = iter(sampler)\n",
    "for _ in range(5):\n",
    "    print(next(sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we create the `DistributedSampler` for the processes belonging to the second Data Parallel Rank. As can be seen, the sampler access different indices from the `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "9\n",
      "13\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "sampler_1 = DistributedSampler(train_nanoset, num_replicas=4, rank=1, shuffle=False)\n",
    "sampler_1 = iter(sampler_1)\n",
    "for _ in range(5):\n",
    "    print(next(sampler_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Collator\n",
    "\n",
    "If we look at the \\_\\_getitem\\_\\_ method of the `Nanoset`, we observe how each sample is a dictionary containing the tokens in a numpy array. But, weren't the model inputs of type `torch.Tensor`? And where are the labels? In the case of having pipeline parallelism, we only need the inputs in the first stage and the labels in the last one, right? All these questions will be addressed in the *collator*.\n",
    "\n",
    "The *collator* will be responsible for carrying out a final data processing step on each sample before feeding them into the model. This final step often depends on more specific variables of each process, such as the Data Parallel Rank or the Pipeline stage. While it is true that we could do this in the \\_\\_getitem\\_\\_ method, it's better to do it in the collator so we construct exactly the same `Nanoset` in all processes.\n",
    "\n",
    "Below we show the *collator* that we will use with the `Nanosets` in `Nanotron`. As can be seen, we will distinguish between the processes at the beginning of the pipeline (which will require the inputs), those at the last stage of the pipeline (which will require the labels), and the rest. It will also be responsible for creating the labels from the inputs (Shifting the tokens 1 position to the right) and transforming all arrays into `torch.Tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "from nanotron.parallel.pipeline_parallel.tensor_pointer import TensorPointer\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DataCollatorForCLM:\n",
    "    \"\"\"\n",
    "    Data collator used for causal language modeling.\n",
    "\n",
    "    - input_pp_rank: Feeds the input_ids to the model\n",
    "    - output_pp_rank: Feeds the labels to the model\n",
    "    - other pp ranks: Don't have data. Instead, we use `TensorPointer` to point to the pipeline rank having the data. Used by Nanotron's pipeline engine.\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_length: int\n",
    "    input_pp_rank: int\n",
    "    output_pp_rank: int\n",
    "    current_pp_rank: int\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, List[np.ndarray]]]) -> Dict[str, Union[torch.Tensor, TensorPointer]]:\n",
    "        # Process the case when current rank doesn't require data. We return `TensorPointer` that points to ranks having the data.\n",
    "        if self.current_pp_rank not in [\n",
    "            self.input_pp_rank,\n",
    "            self.output_pp_rank,\n",
    "        ]:\n",
    "            assert all(len(example) == 0 for example in examples)\n",
    "            return {\n",
    "                \"input_ids\": TensorPointer(group_rank=self.input_pp_rank),\n",
    "                \"input_mask\": TensorPointer(group_rank=self.input_pp_rank),\n",
    "                \"label_ids\": TensorPointer(group_rank=self.output_pp_rank),\n",
    "                \"label_mask\": TensorPointer(group_rank=self.output_pp_rank),\n",
    "            }\n",
    "\n",
    "        # Make sure we load `input_ids` column.\n",
    "        assert all(list(example.keys()) == [\"input_ids\"] for example in examples)\n",
    "\n",
    "        input_ids = np.vstack([examples[i][\"input_ids\"] for i in range(len(examples))])  # (b, s)\n",
    "        batch_size, expanded_input_length = input_ids.shape\n",
    "\n",
    "        result: Dict[str, Union[np.ndarray, TensorPointer]] = {}\n",
    "\n",
    "        result[\"input_ids\"] = TensorPointer(group_rank=self.input_pp_rank)\n",
    "        result[\"input_mask\"] = TensorPointer(group_rank=self.input_pp_rank)\n",
    "        result[\"label_ids\"] = TensorPointer(group_rank=self.output_pp_rank)\n",
    "        result[\"label_mask\"] = TensorPointer(group_rank=self.output_pp_rank)\n",
    "\n",
    "        assert (\n",
    "            expanded_input_length == self.sequence_length + 1\n",
    "        ), f\"Samples should be of length {self.sequence_length + 1} (seq_len+1), but got {expanded_input_length}\"\n",
    "\n",
    "        # Process inputs: last token is the label\n",
    "        if self.current_pp_rank == self.input_pp_rank:\n",
    "            result[\"input_ids\"] = input_ids[:, :-1]\n",
    "            result[\"input_mask\"] = np.ones((batch_size, self.sequence_length), dtype=np.bool_)\n",
    "\n",
    "        # Process labels: shift them to the left\n",
    "        if self.current_pp_rank == self.output_pp_rank:\n",
    "            result[\"label_ids\"] = input_ids[:, 1:]\n",
    "            result[\"label_mask\"] = np.ones((batch_size, self.sequence_length), dtype=np.bool_)\n",
    "\n",
    "        if isinstance(result[\"input_ids\"], torch.Tensor) and result[\"input_ids\"].shape[-1] != self.sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"`labels` are incorrectly preprocessed. `labels` length is {result['input_ids'].shape[-1]}, but should be\"\n",
    "                f\" {self.sequence_length}.\"\n",
    "            )\n",
    "        if isinstance(result[\"label_ids\"], torch.Tensor) and result[\"label_ids\"].shape[-1] != self.sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"`labels` are incorrectly preprocessed. `labels` length is {result['label_ids'].shape[-1]}, but should be\"\n",
    "                f\" {self.sequence_length}.\"\n",
    "            )\n",
    "\n",
    "        # Cast np.array to torch.Tensor\n",
    "        result = {k: v if isinstance(v, TensorPointer) else torch.from_numpy(v) for k, v in result.items()}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we create a *collator* and an example of what it will produce from each sample extracted from the `Nanoset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[331, 141,  27,  ..., 632, 494, 340]]),\n",
       " 'input_mask': tensor([[True, True, True,  ..., True, True, True]]),\n",
       " 'label_ids': tensor([[141,  27, 187,  ..., 494, 340, 902]]),\n",
       " 'label_mask': tensor([[True, True, True,  ..., True, True, True]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForCLM(sequence_length=SEQUENCE_LENGTH, input_pp_rank=0, output_pp_rank=0, current_pp_rank=0)\n",
    "collator([{\"input_ids\": np.random.randint(0, 1000, size=(SEQUENCE_LENGTH +1,))}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the `Dataset` from which we will extract samples, the *sampler* containing which samples to extract, and the *collator* that will take care of formatting the samples to feed them into the model, we can finally jump into building the DataLoader! We simply need to input these 3 elements along with the desired `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_nanoset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the contents of each batch that we will input into the model and the dimensions of the most relevant tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[19978, 21420,   543,  ..., 15494,   355,   257],\n",
      "        [  257,  1256,  1342,  ...,  1424,   379,   428],\n",
      "        [  460,   470,  4255,  ...,   518,   373,   922],\n",
      "        [  326,   314,  4444,  ...,   340,   338,   287]], dtype=torch.int32), 'input_mask': tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]]), 'label_ids': tensor([[21420,   543,  1234,  ...,   355,   257, 21239],\n",
      "        [ 1256,  1342, 38635,  ...,   379,   428,  4067],\n",
      "        [  470,  4255,  2861,  ...,   373,   922,    13],\n",
      "        [  314,  4444,   510,  ...,   338,   287,   262]], dtype=torch.int32), 'label_mask': tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])}\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = iter(train_dataloader)\n",
    "batch = next(train_dataloader)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] input_ids shape: torch.Size([4, 8192]). label_ids shape: torch.Size([4, 8192])\n",
      "[Batch 1] input_ids shape: torch.Size([4, 8192]). label_ids shape: torch.Size([4, 8192])\n",
      "[Batch 2] input_ids shape: torch.Size([4, 8192]). label_ids shape: torch.Size([4, 8192])\n",
      "[Batch 3] input_ids shape: torch.Size([4, 8192]). label_ids shape: torch.Size([4, 8192])\n"
     ]
    }
   ],
   "source": [
    "for idx in range(4):\n",
    "    batch = next(train_dataloader)\n",
    "    print(f'[Batch {idx}] input_ids shape: {batch[\"input_ids\"].shape}. label_ids shape: {batch[\"label_ids\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend that you consult the [documentation of Torch's `DataLoader`s](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) and explore other parameters such as `num_workers`, `drop_last`, or `pin_memory`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpointing and resuming training\n",
    "\n",
    "During training, interruptions are possible (such as GPU crashing, exceeding the time limit of the cluster policy, cluster down for maintenance, etc.), and there's nothing worse than having to start training all over again, right? That's why it's essential to program checkpointing mechanisms that store the model's state, but also of the `DataLoader`'s states in order to avoid consuming the same data again. To achieve this, we will have to store the number of samples consumed at the moment at which we took the checkpoint along with the model and, once we resume training, skip this number of samples in the *sampler*.\n",
    "\n",
    "Below, we show an example of how training is stopped at iteration 18 when we were going to consume sample 70. And how, upon resuming training, with the help of the `SkipBatchSampler`, we skip the first `17*batch_size` samples to resume training from sample 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "init_sampler = DistributedSampler(train_nanoset, num_replicas=4, rank=2, shuffle=False)\n",
    "\n",
    "CRASH_AT_ITERATION = 17\n",
    "\n",
    "for iter, sample in enumerate(init_sampler):\n",
    "    if iter == CRASH_AT_ITERATION:\n",
    "        break\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler\n",
    "\n",
    "class SkipBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    A `torch.utils.data.BatchSampler` that skips the first `n` batches of another `torch.utils.data.BatchSampler`.\n",
    "    Note that in case of DDP, we skip batches on each rank, so a total of `skip_batches * parallel_context.dp_pg.size()` batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_sampler: BatchSampler, skip_batches: int, dp_size: int):\n",
    "        self.batch_sampler = batch_sampler\n",
    "        # In case of DDP, we skip batches on each rank, so a total of `skip_batches * parallel_context.dp_pg.size()` batches\n",
    "        self.skip_batches = skip_batches // dp_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index, samples in enumerate(self.batch_sampler):\n",
    "            if index >= self.skip_batches:\n",
    "                yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "resume_sampler = DistributedSampler(train_nanoset, num_replicas=4, rank=2, shuffle=False)\n",
    "resume_sampler = SkipBatchSampler(resume_sampler, 4*CRASH_AT_ITERATION, 4)\n",
    "\n",
    "for sample in resume_sampler:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To sum up\n",
    "Throughout this post, we have seen the fundamental aspects of `Datasets` and `DataLoaders`, introducing `Nanoset`s, a `Dataset` for training LLMs at scale.\n",
    "\n",
    "In summary, to create a `DataLoader` we need:\n",
    "- `Dataset`: An object that we can index to extract samples. In environments with Data Parallelism, we will create the same `Dataset` in all processes. We should pay attention to the following methods:\n",
    "  - **\\_\\_init\\_\\_**: Method invoked during the object creation where we will establish its attributes and build the necessary logic to extract samples.\n",
    "  - **\\_\\_getitem\\_\\_**: Method to extract samples from the `Dataset`. This is where we will carry out more costly operations such as IO or preprocessing.\n",
    "  - **\\_\\_len\\_\\_**: Method that returns the number of samples we can query to the `Dataset`, essential for the `DataLoader`'s *sampler*.\n",
    "- *Sampler*: It will contain the indices that the `DataLoader` will extract from the `Dataset`. With Data Parallelism, we will use the `DistributedSampler`, which will divide the `Dataset` indices among the different Data Parallel Ranks. To resume training properly, it will be necessary to store the model checkpoint along with the number of total samples consumed.\n",
    "- *Collator*: Responsible for the final step of data processing before feeding batches of data into the model.\n",
    "\n",
    "With all of this, you are now ready to construct any `DataLoader` you propose ðŸ¥³! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/DataLoader_Diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soon, I will publish a second part introducing Torch's [`IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)s, which allows us to stream and tokenize text on the fly but requires additional logic both to build the batches and to resume training after an interruption (Fundamental for large-scale training)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
